title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins") +
ggplot2::theme_minimal() +
#ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins") +
#ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold"))
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins")
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet    "
)
ts_plot(rt %>% filter(created_at > "2020-03-28 22:35:52"), by = "mins") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
# load libraries
library(tidytext)
library(tm)
library(dplyr)
library(textclean)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(wordcloud)
library(topicmodels)
library(textdata)
library(rtweet)
rt <- search_tweets(
# Set search keyword
q = "#COVID19",
# Set the number of tweets to return
n = 18000,
# Specify the type of search results
type = "mixed",
# Do not include retweets
include_rts = FALSE,
# Set tweet language to English
lang = "en",
# Twitter rate limits cap the number of search results returned to 18,000 every 15 minutes. To request more than that, simply set retryonratelimit = TRUE and rtweet will wait for rate limit resets.
retryonratelimit = TRUE
)
View(rt)
ts_plot(rt %>% filter(created_at > "2020-03-28 22:00:00"), by = "mins") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
head(rt)
head(sort(rt, by = "created_at"))
head(rt(order(created_at)))
head(rt[order(created_at)])
head(rt[order(created_at)],)
rt[order(created_at)],)
rt[order(created_at),])
head(rt[order(created_at),])
head(rt[order("created_at"),])
head(rt[order("created_at"),], 5)
head(rt[order(rt$created_at),])
head(rt[order(rt$created_at),], 10)
tail(rt[order(rt$created_at),], 10)
dim(rt)
# transform our data to source format
covid <- VectorSource(rt$text)
# check the first tweet
covid[1]
# save the returned data to csv
write.csv(rt, "covid19.csv")
# save the returned data to csv
write.csv(rt, "covid19.csv", row.names = FALSE)
class(rt)
# save the returned data to csv
write.table(rt, "covid19.csv")
# save the returned data to csv
write.csv(rt %>% unlist(), "covid19.csv")
# transform the source file into a Corpus
cv_corpus <- VCorpus(covid)
# transform the source file into a Corpus
cv_corpus <- VCorpus(covid)
# then inspect the first one tweet
inspect(cv_corpus[1])
clean_corpus <- function(corp) {
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
print("Before")
print(content(corpus[[1]]))
print("After")
print(content(cleaned_data[[1]]))
# print the first tweet after cleaning
print("Before")
print(content(corpus[[1]]))
cat("After:\n", content(cv_corpus_cleaned[[1]]))
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]))
cat("After:\n", content(cv_corpus_cleaned[[1]]))
clean_corpus <- function(corp) {
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# transform all characters to lower case
#corp <- tm_map(corp, content_transformer(tolower))
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
clean_corpus <- function(corp) {
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# word stemming
corp <- tm_map(corp, PlainTextDocument)
corp <- tm_map(corp, stemDocument)
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
clean_corpus <- function(corp) {
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# word stemming
corp <- tm_map(corp, PlainTextDocument)
corp <- tm_map(corp, stemDocument)
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus", "covid"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
# transform the cleaned data into a doc-term matrix
dt_cv_corpus <- DocumentTermMatrix(cv_corpus_cleaned)
# print the dimension of the matrix
dim(dt_cv_corpus)
# transform to matrix format
mt_cv_corpus <- as.matrix(dt_cv_corpus)
# view a portion of the matrix
mt_cv_corpus[1:10, 30:35]
# view a portion of the matrix
mt_cv_corpus[1:10, 1:35]
# view a portion of the matrix
mt_cv_corpus[1:10, 50:55]
# visualise twitter data
# convert from matrix to tidy data
tidy_data <- tidy(mt_cv_corpus)
# visualise twitter data
# convert from matrix to tidy data
tidy_data <- tidy(dt_cv_corpus)
tidy_data
term_fre <- colSums(mt_cv_corpus)
term_fre <- sort(term_fre, decreasing = TRUE)
term_fre[1:20]
clean_corpus <- function(corp) {
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# word stemming
corp <- tm_map(corp, PlainTextDocument)
corp <- tm_map(corp, stemDocument)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus", "covid"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
# transform the cleaned data into a doc-term matrix
dt_cv_corpus <- DocumentTermMatrix(cv_corpus_cleaned)
# print the dimension of the matrix
dim(dt_cv_corpus)
# transform to matrix format
mt_cv_corpus <- as.matrix(dt_cv_corpus)
# view a portion of the matrix
mt_cv_corpus[1:10, 50:55]
# transform to matrix format
mt_cv_corpus <- as.matrix(dt_cv_corpus)
# load libraries
library(tidytext)
library(tm)
library(dplyr)
library(textclean)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(wordcloud)
library(topicmodels)
library(textdata)
library(rtweet)
# load libraries
library(tidytext)
library(tm)
library(dplyr)
library(textclean)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(wordcloud)
library(topicmodels)
library(textdata)
library(rtweet)
rt <- read.csv(file = "covid19.csv", stringsAsFactors = FALSE)
# check the dimension of returned tweets data
dim(rt)
# check the dimension of returned tweets data
dim(rt)
# View the dataframe
View(rt)
rt <- search_tweets(
# Set search keyword
q = "#COVID19",
# Set the number of tweets to return
n = 18000,
# Specify the type of search results
type = "mixed",
# Do not include retweets
include_rts = FALSE,
# Set tweet language to English
lang = "en",
# Twitter rate limits cap the number of search results returned to 18,000 every 15 minutes. To request more than that, simply set retryonratelimit = TRUE and rtweet will wait for rate limit resets.
retryonratelimit = TRUE
)
# check the dimension of returned tweets data
dim(rt)
# save the returned data to csv
save_as_csv(rt, "covid19.csv")
rt2 <- read.csv(file = "covid19.csv", stringsAsFactors = FALSE)
# check the dimension of returned tweets data
dim(rt2)
# load libraries
library(tidytext)
library(tm)
library(dplyr)
library(textclean)
library(ggplot2)
library(ggthemes)
library(reshape2)
library(wordcloud)
library(topicmodels)
library(textdata)
library(rtweet)
rt <- search_tweets(
# Set search keyword
q = "#COVID19",
# Set the number of tweets to return
n = 18000,
# Specify the type of search results
type = "mixed",
# Do not include retweets
include_rts = FALSE,
# Set tweet language to English
lang = "en",
# Twitter rate limits cap the number of search results returned to 18,000 every 15 minutes. To request more than that, simply set retryonratelimit = TRUE and rtweet will wait for rate limit resets.
retryonratelimit = TRUE
)
# check the dimension of returned tweets data
dim(rt1)
# check the dimension of returned tweets data
dim(rt)
# check the dimension of returned tweets data
dim(rt2)
View(rt2)
# check the dimension of returned tweets data
dim(rt)
# check the first 10 rows by ordering created_at time
head(rt[order(rt$created_at),], 10)
# check the last 10 rows by ordering created_at time
tail(rt[order(rt$created_at),], 10)
# Plot the frequency of tweets after 2020-03-28 22:00:00 with interval 1 min
ts_plot(rt %>% filter(created_at > "2020-03-28 22:00:00"), by = "mins") +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
# Plot the frequency of tweets after 2020-03-28 22:00:00 with interval 1 min
ts_plot(rt %>% filter(created_at > "2020-03-28 22:00:00"), by = "mins", trim = 1) +
ggplot2::theme_minimal() +
ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
ggplot2::labs(
x = NULL, y = NULL,
title = "Frequency of #COVID19 tweets from past hours",
subtitle = "Twitters counts aggregated using 1 mins intervals",
caption = "\nSource: Data collected from Twitter's REST API via rtweet"
)
# transform our data to source format
covid <- VectorSource(rt$text)
# check the first tweet
covid[1]
# transform the source file into a Corpus
cv_corpus <- VCorpus(covid)
# then inspect the first one tweet
inspect(cv_corpus[1])
clean_corpus <- function(corp) {
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# word stemming
corp <- tm_map(corp, stemDocument)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus", "covid"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
clean_corpus <- function(corp) {
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# word stemming
corp <- tm_map(corp, stemDocument)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus", "covid"))
for (i in seq(corp)) {
corp[[i]] <- gsub('[^a-zA-Z|[:blank:]]', "", corp[[i]])
}
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
cv_corpus_cleaned <- clean_corpus(cv_corpus)
clean_corpus <- function(corp) {
# transform all characters to lower case
corp <- tm_map(corp, content_transformer(tolower))
# word stemming
corp <- tm_map(corp, stemDocument)
# remove numbers
corp <- tm_map(corp, removeNumbers)
# remove punctuation
corp <- tm_map(corp, removePunctuation)
# remove stop words and "apple"
corp <- tm_map(corp, removeWords, c(stopwords("en"), "covid19", "coronavirus", "covid"))
# strip extra whitespace
corp <- tm_map(corp, stripWhitespace)
# remove special characters
#f <- content_transformer(function(x, pattern) gsub(pattern, "", x))
#corp <- tm_map(corp, f, "\”“")
# [\[\]"',.^&$%#]
# remove any html format if any
corp <- tm_map(corp, content_transformer(replace_html))
# remove any URLs
corp <- tm_map(corp, content_transformer(replace_url))
return(corp)
}
cv_corpus_cleaned <- clean_corpus(cv_corpus)
# print the first tweet after cleaning
cat("Before:\n", content(cv_corpus[[1]]), "\n\n")
cat("After:\n", content(cv_corpus_cleaned[[1]]))
# transform the cleaned data into a doc-term matrix
dt_cv_corpus <- TermDocumentMatrix(cv_corpus_cleaned)
# print the dimension of the matrix
dim(dt_cv_corpus)
# transform to matrix format
mt_cv_corpus <- as.matrix(dt_cv_corpus)
# transform to matrix format
mt_cv_corpus <- as.matrix(dt_cv_corpus)
