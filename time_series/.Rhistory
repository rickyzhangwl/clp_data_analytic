ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# create association rules with support 0.002, confidence 0.65
rules_1 <- apriori(df_tr, parameter = list(supp = 0.002, conf = 0.7))
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# inspect first few transactions
inspect(head(df_tr))
# Display the distribution of number of items per basket
hist(size(df_tr), breaks = 0:80, xaxt = "n",
main = "Number of Items per Basket", xlab = "Number of Items", col = "#00ADB5")
axis(side = 1, at = seq(0, 80, by = 10), cex.axis = 0.8) # set x-axis label
mtext(paste("Total Baskets: ", length(df_tr)), line = -1) # set sub-title
itemFrequencyPlot(df_tr, topN = 15, type = "absolute", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Absolute")
itemFrequencyPlot(df_tr, topN = 15, type = "relative", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Relative")
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# Plot the top 10 itemsets
plot(sort(ft_rules, by = "support", decreasing = T)[1:10], method = "graph", control = list(cex=0.6), main = "Top 10 Most Frequent Bought-Together Itemsets")
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# combine the four datasets with different support level
num_rules <- data.frame(d[["1"]], d[["2"]], d[["3"]], d[["4"]], confidence_levels)
# plot the rules numbers in different support and confidence levels
ggplot(num_rules, aes(x = confidence_levels)) +
geom_line(aes(y = d[["1"]], color = paste("Support level of", support_levels[1]))) +
geom_point(aes(y = d[["1"]], color = paste("Support level of", support_levels[1]))) +
geom_line(aes(y = d[["2"]], color = paste("Support level of", support_levels[2]))) +
geom_point(aes(y = d[["2"]], color = paste("Support level of", support_levels[2]))) +
geom_line(aes(y = d[["3"]], color = paste("Support level of", support_levels[3]))) +
geom_point(aes(y = d[["3"]], color = paste("Support level of", support_levels[3]))) +
geom_line(aes(y = d[["4"]], color = paste("Support level of", support_levels[4]))) +
geom_point(aes(y = d[["4"]], color = paste("Support level of", support_levels[4]))) +
scale_x_continuous(breaks = seq(0.65, 0.9, by = 0.05), # label the confidence values
labels = seq(0.65, 0.9, by = 0.05)) +
labs(title = "Number of Rules by Support & Confidence",
x = "Confidence", y = "Count") +
theme_bw() + # set theme to blankwhite
theme(legend.title = element_blank())
# create association rules with support 0.002, confidence 0.65
rules_1 <- apriori(df_tr, parameter = list(supp = 0.002, conf = 0.7))
inspect(rules_1)
# filter and sort the rules with "Banana" in right hand side
bf_bnn <- sort(subset(rules_1, subset = rhs %pin% "Banana"), by = "confidence", decreasing = T)
inspect(bf_bnn)
plot(bf_bnn, method = "graph", control = list(cex=0.5),
main = "The Items Customers likely Purchase Before Buying Banana")
# filter the rules without Banana on rhs
other_rule <- sort(subset(rules_1, !(rhs %pin% "Banana")), by = "confidence", decreasing = T)
inspect(other_rule)
# List all the 9 rules by confidence level from high to low
selected_rules <- DATAFRAME(rules_1) %>%
arrange(desc(confidence))
recomm <- paste(selected_rules$LHS, "=>", selected_rules$RHS)
recomm
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
recomm
install.packages("tinytex")
install.packages("rmarkdown")
install.packages("rmarkdown")
class(Boston)
data(Boston)
library(Boston)
library(MASS)
boston <- Boston
str(boston)
summary(boston)
dim(boston)
str(boston)
?Boston
head(boston)
boston$black <- NULL
set.seed(198761)
sample <- sample.int(n = nrow(boston), size = floor(0.75 * nrow(boston)))
train <- boston[sample, ]
test <- boston[-sample, ]
set.seed(198761)
sample <- sample.int(n = nrow(boston), size = floor(0.75 * nrow(boston)))
train <- boston[sample, ]
test <- boston[-sample,]
cor(train)
corplot(train)
cor(train)
library(ggplot)
library(ggplot2)
library(ggplot2)
ggplot(boston, aes(x=rm, y=medv)) + geom_point()
ggplot(boston, aes(x=rm, y=medv)) + geom_point() + geom_smooth(method = 'lm)
ggplot(boston, aes(x=rm, y=medv)) + geom_point() + geom_smooth(method = 'lm')
corplot(boston)
univariate_model <- lm(medv ~ rm, boston)
univariate_model <- lm(medv ~ rm, boston)
summary(univariate_model)
plot(univariate_model)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'blue', alpha = 0.5)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'dark blue', alpha = 0.5)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'dark blue', alpha = 0.5, bins = 40)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'dark blue', alpha = 0.5, binwidth = 2)
test$predicted.medv <- predict(univariate_model, test)
view(test)
test$predicted.medv <- predict(univariate_model, test)
library(dplyr)
view(test)
test$predicted.medv <- predict(univariate_model, test)
library(tidyverse)
view(test)
test$predicted.medv <- predict(univariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
multivariate_model <- lm(medv ~ ., train)
plot(multivariate_model)
test$predicted.medv <- predict.lm(multivariate_model, test)
view(test)
test$predicted.medv <- predict(multivariate_model, test)
view(test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
annova(univariate_model, multivariate_model)
anova(univariate_model, multivariate_model)
univariate_model <- lm(medv ~ rm, test)
summary(univariate_model)
test$predicted.medv <- predict(univariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
anova(univariate_model, multivariate_model)
anova(univariate_model, multivariate_model)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
test$predicted.medv <- predict(multivariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
anova(univariate_model, multivariate_model)
univariate_model <- lm(medv ~ rm, train)
summary(univariate_model)
test$predicted.medv <- predict(univariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
anova(univariate_model, multivariate_model)
m1 <- stepAIC(multivariate_model, direction = 'both)
m1 <- stepAIC(multivariate_model, direction = 'both')
d[[toString(k)]] <- rules_count
# Load required packages
library(dplyr)
library(arules)
library(arulesViz)
library(ggplot2)
# Load the data files
orders <- read.csv("order_products__prior.csv", nrows = 30000) # to reduce computability, we just load the first 30000 rows.
# randomly sample the rows
products <- read.csv("products.csv")
# Check the data structure
glimpse(orders)
glimpse(products)
# Inspect first few rows
head(orders)
head(products)
# Join the data on product_id
df <- orders %>%
inner_join(products, by = "product_id") %>%
group_by(order_id) %>%
summarise(basket = as.vector(list(product_name)))
# check the cleaned data
head(df)
# convert dataframe to sparse matrix for further analysis
df_tr <- as(df$basket, "transactions")
# check how many transactions and items
df_tr
# inspect first few transactions
inspect(head(df_tr))
# Display the distribution of number of items per basket
hist(size(df_tr), breaks = 0:80, xaxt = "n",
main = "Number of Items per Basket", xlab = "Number of Items", col = "#00ADB5")
axis(side = 1, at = seq(0, 80, by = 10), cex.axis = 0.8) # set x-axis label
mtext(paste("Total Baskets: ", length(df_tr)), line = -1) # set sub-title
itemFrequencyPlot(df_tr, topN = 15, type = "absolute", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Absolute")
itemFrequencyPlot(df_tr, topN = 15, type = "relative", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Relative")
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# Plot the top 10 itemsets
plot(sort(ft_rules, by = "support", decreasing = T)[1:10], method = "graph", control = list(cex=0.6), main = "Top 10 Most Frequent Bought-Together Itemsets")
# Load packages
library(tidyverse)
# 2. Load data file
df <- read_csv("district_data.csv")
# check first a few rows
head(df)
# check the dimension of data
dim(df)
# check the summary of data
summary(df)
# check column names
names(df)
# rename the column names
names(df) <- c("district", "avg_income", "median_age")
names(df)
# remove $ and thousand separator
df$avg_income <- gsub("[$,]", "", df$avg_income)
# convert to numeric
df$avg_income <- as.numeric(df$avg_income)
# check the column types
glimpse(df)
qplot(x = df$median_age, y = df$avg_income, label = df$district, geom = "text")
df_scaled <- as.data.frame(scale(df[c("median_age", "avg_income")]))
head(df_scaled)
# define the number of clusters
k_clusters <- 3
# get clustering result
k_means_result <- kmeans(df_scaled, centers = k_clusters)
# add cluster result to original data
df$cluster <- as.factor(k_means_result$cluster)
head(df)
# visualize cluster result
qplot(x = df$median_age, y = df$avg_income, label = df$district, geom = "text", color = df$cluster)
# set maximum number of cluster
k_cluster_max <- 10
# iterate through the number of clusters from 1 to 10
within_sum_of_squares <- sapply(1:k_cluster_max,
function(k){
kmeans(df_scaled, centers = k)$tot.withinss})
# visualize the output
plot(1:k_cluster_max, within_sum_of_squares, type = "b",
xlab = "Number of clusters", ylab = "Total within cluster sum of squares")
# conduct clustering with 4 clusters
k_means_result_2 <- kmeans(df_scaled, centers = 4)
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# visualize cluster result with ggrepel to avoid label overlapping
library(ggrepel)
qplot(x = df$median_age, y = df$avg_income, label = df$district, color = df$cluster,
xlab = "Median Age", ylab = "Average Income") + geom_text_repel()
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# conduct clustering with 4 clusters
k_means_result_2 <- kmeans(df_scaled, centers = 4)
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# visualize cluster result with ggrepel to avoid label overlapping
library(ggrepel)
qplot(x = df$median_age, y = df$avg_income, label = df$district, color = df$cluster,
xlab = "Median Age", ylab = "Average Income") + geom_text_repel()
# conduct clustering with 4 clusters
k_means_result_2 <- kmeans(df_scaled, centers = 4)
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# visualize cluster result with ggrepel to avoid label overlapping
library(ggrepel)
qplot(x = df$median_age, y = df$avg_income, label = df$district, color = df$cluster,
xlab = "Median Age", ylab = "Average Income") + geom_text_repel()
install.packages(c("arules", "clipr", "curl", "DT", "foreach", "foreign", "ggplot2", "hms", "httr", "iterators", "knitr", "markdown", "modelr", "nlme", "Rcpp", "rmarkdown", "rpart.plot", "seriation", "sys", "visNetwork", "whisker", "xfun", "xml2"))
install.packages(c("arules", "clipr", "curl", "DT", "foreach", "foreign", "ggplot2", "hms", "httr", "iterators", "knitr", "markdown", "modelr", "nlme", "Rcpp", "rmarkdown", "rpart.plot", "seriation", "sys", "visNetwork", "whisker", "xfun", "xml2"))
install.packages(c("arules", "ggplot2", "markdown"))
install.packages(c("DT", "httr"))
install.packages(c("clipr", "curl"))
install.packages(c("foreach", "foreign", "hms", "iterators", "knitr", "modelr", "nlme", "Rcpp", "rmarkdown", "rpart.plot", "seriation", "sys", "visNetwork", "whisker", "xfun", "xml2"))
setwd("~/GitHub/clp_data_analytic/time_series")
# Automated forecasting using an ETS model
ets <- ets(ts_domestic)
# Load packages
library(tidyverse)
library(xts)
library(forecast)
library(tseries)
library(ggfortify)
# read the data
df <- read.csv("hk_electricity.csv")
# inspect the first and last few rows
head(df)
tail(df)
# Count the number of rows
nrow(df)
# Split the domestic consumption data into training set and test set
domestic_train <- head(df$Domestic, nrow(df) - 10)
domestic_test <- tail(df$Domestic, 10)
# change data to time series format
ts_domestic <- ts(df$Domestic, frequency = 12, start = c(1970, 1), end = c(2019, 10))
ts_domestic_train <- ts(domestic_train, frequency = 12, start = c(1970, 1), end = c(2018, 12))
ts_domestic_test <- ts(domestic_test, frequency = 12, start = c(2019, 1))
# Check how many observations there are for each unit of time
frequency(ts_domestic_train)
# Get a summary of the values
summary(ts_domestic_train)
# Plot the training set and add a linear regression line
plot(ts_domestic_train, ylab = "Electricity Consumption (in Terajoules)", main = "HK Domestic Electricity Consumption 1970-2018")
abline(reg=lm(ts_domestic_train~time(ts_domestic_train)))
# Explore some seasonality within our data using a boxplot
boxplot(ts_domestic_train~cycle(ts_domestic_train), xlab = "Month", ylab = "Domestic electricity consumption")
# Decompose the data
decomposed <- decompose(ts_domestic_train, type = "multiplicative")
autoplot(decomposed)
arima <- auto.arima(ts_domestic_train)
arima
#T he ggtsdiag function from ggfortify R package performs model diagnostics of the residuals and the acf. will include a autocovariance plot.
ggtsdiag(arima)
# We can plot a forecast of the time series using the arima model, with a 95% confidence interval, forecasted data till Oct 2019.
arima_forecast <- forecast(arima, h=10, level = c(95)) # 10 for 2019.01-2019.10
autoplot(arima_forecast, main = ("HK Domestic Electricity Consumption 1970-2020")) + ylab("Electricity Consumption (in Terajoules)")
# visualize the model overall accuracy
plot((arima$x), col = "red")
lines((arima$fitted), col = "blue")
# Automated forecasting using an ETS model
ets <- ets(ts_domestic)
ets
# The ggtsdiag function from ggfortify R package performs model diagnostics of the residuals and the acf. will include a autocovariance plot.
ggtsdiag(ets)
# We can plot a forecast of the time series using the ETS model, with a 95% confidence interval, forecasted data till Oct 2019
ets_forecast <- forecast(ets, h=12, level = c(95)) # 12 for 2019.01-2019.12
autoplot(ets_forecast, main = ("HK Domestic Electricity Consumption 1970-2020")) + ylab("Electricity Consumption (in Terajoules)")
# Automated forecasting using an ETS model
ets <- ets(ts_domestic_train)
ets
# The ggtsdiag function from ggfortify R package performs model diagnostics of the residuals and the acf. will include a autocovariance plot.
ggtsdiag(ets)
# We can plot a forecast of the time series using the ETS model, with a 95% confidence interval, forecasted data till Oct 2019
ets_forecast <- forecast(ets, h=10, level = c(95)) # 10 for 2019.01-2019.10
autoplot(ets_forecast, main = ("HK Domestic Electricity Consumption 1970-2020")) + ylab("Electricity Consumption (in Terajoules)")
# Compare the forecasted data to actual data (2019.1-2019.10)
ts.union(ts_domestic_test, head(arima_forecast$mean,10), head(ets_forecast$mean,10))
# Plot the comparison
plot(ts_domestic_test, ylab = "Domestic Electricity Consumption")
lines(head(arima_forecast$mean,10), col="blue")
lines(head(ets_forecast$mean,10), col="green")
arima_forecast %>% forecast(h = 10) %>%
accuracy(ts_domestic)
ets_forecast %>% forecast(h = 10) %>%
accuracy(ts_domestic)
arima_2 <- auto.arima(ts_domestic)
forecast_2021 <- forecast(arima_2, h=24, level = c(95))
autoplot(forecast_2021)
arima_2 <- auto.arima(ts_domestic)
forecast_2021 <- forecast(arima_2, h=2+24, level = c(95))
autoplot(forecast_2021)
# check the forecast data
forecast_2021$mean
ts_commercial <- ts(df$Commercial, frequency = 12, start = c(1970, 1), end = c(2019, 10))
ts_industrial <- ts(df$Industrial, frequency = 12, start = c(1970, 1), end = c(2019, 10))
ts_streetlight <- ts(df$Street.lighting, frequency = 12, start = c(1970, 1), end = c(2019, 10))
arima_commercial <- auto.arima(ts_commercial)
forecast_2021_commercial <- forecast(arima_commercial, h=2+24, level = c(95))
autoplot(forecast_2021_commercial)
forecast_2021_commercial$mean
# create commercial electricity tim-series data
ts_industrial <- ts(df$Industrial, frequency = 12, start = c(1970, 1), end = c(2019, 10))
# use auto arima model to forecast the data till the end of 2021
arima_industrial <- auto.arima(ts_industrial)
forecast_2021_industrial <- forecast(arima_industrial, h=2+24, level = c(95))
# plot the data
autoplot(forecast_2021_industrial)
# check the forecast value of industrial electricity consumption
forecast_2021_industrial$mean
# check the forecast value of industrial electricity consumption
forecast_2021_industrial$series
# check the forecast value of industrial electricity consumption
forecast_2021_industrial$upper
# check the forecast value of industrial electricity consumption
forecast_2021_industrial$fitted
write.zoo(forecast_2021_commercial$fitted, "commercial")
write.zoo(forecast_2021_commercial$fitted, "commercial.csv")
write.csv(forecast_2021_commercial$fitted, "commercial")
write.csv(forecast_2021_commercial$fitted, "commercial.csv")
write.csv(forecast_2021_commercial$mean, "commercial.csv")
# check the forecast value of industrial electricity consumption
forecast_2021_industrial$mean
# create commercial electricity tim-series data
ts_streetlight <- ts(df$Street.lighting, frequency = 12, start = c(1970, 1), end = c(2019, 10))
# use auto arima model to forecast the data till the end of 2021
arima_streetlight <- auto.arima(ts_streetlight)
forecast_2021_streetlight <- forecast(arima_streetlight, h=2+24, level = c(95))
# plot the data
autoplot(forecast_2021_streetlight)
plot(ts_commercial)
plot(ts_streetlight)
plot(ts_industrial)
write.csv(forecast_2021$mean, "domestic.csv")w
write.csv(forecast_2021$mean, "domestic.csv")
write.csv(forecast_2021_commercial$mean, "commercial.csv")
write.csv(forecast_2021_industrial$mean, "industrial.csv")
knitr::include_graphics('/Users/RickyZhang/Downloads/Dashboard 1.png')
# Load packages
library(tidyverse)
library(xts)
library(forecast)
library(tseries)
library(ggfortify)
knitr::include_graphics('/Users/RickyZhang/Downloads/Dashboard 1.png')
# create commercial electricity tim-series data
ts_commercial <- ts(df$Commercial, frequency = 12, start = c(1970, 1), end = c(2019, 10))
# use auto arima model to forecast the data till the end of 2021
arima_commercial <- auto.arima(ts_commercial)
forecast_2021_commercial <- forecast(arima_commercial, h=2+24, level = c(95))
# plot the data
autoplot(forecast_2021_commercial)
# check the forecast value of commercial electricity consumption
forecast_2021_commercial$mean
# create commercial electricity tim-series data
ts_industrial <- ts(df$Industrial, frequency = 12, start = c(1970, 1), end = c(2019, 10))
# use auto arima model to forecast the data till the end of 2021
arima_industrial <- auto.arima(ts_industrial)
forecast_2021_industrial <- forecast(arima_industrial, h=2+24, level = c(95))
# plot the data
autoplot(forecast_2021_industrial)
# check the forecast value of industrial electricity consumption
forecast_2021_industrial$mean
# create commercial electricity tim-series data
ts_streetlight <- ts(df$Street.lighting, frequency = 12, start = c(1970, 1), end = c(2019, 10))
# use auto arima model to forecast the data till the end of 2021
arima_streetlight <- auto.arima(ts_streetlight)
forecast_2021_streetlight <- forecast(arima_streetlight, h=2+24, level = c(95))
# plot the data
autoplot(forecast_2021_streetlight)
# read the data
df <- read.csv("hk_electricity.csv")
# inspect the first and last few rows
head(df)
tail(df)
# Count the number of rows
nrow(df)
