for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# create association rules with support 0.002, confidence 0.65
rules_1 <- apriori(df_tr, parameter = list(supp = 0.002, conf = 0.7))
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# inspect first few transactions
inspect(head(df_tr))
# Display the distribution of number of items per basket
hist(size(df_tr), breaks = 0:80, xaxt = "n",
main = "Number of Items per Basket", xlab = "Number of Items", col = "#00ADB5")
axis(side = 1, at = seq(0, 80, by = 10), cex.axis = 0.8) # set x-axis label
mtext(paste("Total Baskets: ", length(df_tr)), line = -1) # set sub-title
itemFrequencyPlot(df_tr, topN = 15, type = "absolute", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Absolute")
itemFrequencyPlot(df_tr, topN = 15, type = "relative", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Relative")
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# Plot the top 10 itemsets
plot(sort(ft_rules, by = "support", decreasing = T)[1:10], method = "graph", control = list(cex=0.6), main = "Top 10 Most Frequent Bought-Together Itemsets")
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
# combine the four datasets with different support level
num_rules <- data.frame(d[["1"]], d[["2"]], d[["3"]], d[["4"]], confidence_levels)
# plot the rules numbers in different support and confidence levels
ggplot(num_rules, aes(x = confidence_levels)) +
geom_line(aes(y = d[["1"]], color = paste("Support level of", support_levels[1]))) +
geom_point(aes(y = d[["1"]], color = paste("Support level of", support_levels[1]))) +
geom_line(aes(y = d[["2"]], color = paste("Support level of", support_levels[2]))) +
geom_point(aes(y = d[["2"]], color = paste("Support level of", support_levels[2]))) +
geom_line(aes(y = d[["3"]], color = paste("Support level of", support_levels[3]))) +
geom_point(aes(y = d[["3"]], color = paste("Support level of", support_levels[3]))) +
geom_line(aes(y = d[["4"]], color = paste("Support level of", support_levels[4]))) +
geom_point(aes(y = d[["4"]], color = paste("Support level of", support_levels[4]))) +
scale_x_continuous(breaks = seq(0.65, 0.9, by = 0.05), # label the confidence values
labels = seq(0.65, 0.9, by = 0.05)) +
labs(title = "Number of Rules by Support & Confidence",
x = "Confidence", y = "Count") +
theme_bw() + # set theme to blankwhite
theme(legend.title = element_blank())
# create association rules with support 0.002, confidence 0.65
rules_1 <- apriori(df_tr, parameter = list(supp = 0.002, conf = 0.7))
inspect(rules_1)
# filter and sort the rules with "Banana" in right hand side
bf_bnn <- sort(subset(rules_1, subset = rhs %pin% "Banana"), by = "confidence", decreasing = T)
inspect(bf_bnn)
plot(bf_bnn, method = "graph", control = list(cex=0.5),
main = "The Items Customers likely Purchase Before Buying Banana")
# filter the rules without Banana on rhs
other_rule <- sort(subset(rules_1, !(rhs %pin% "Banana")), by = "confidence", decreasing = T)
inspect(other_rule)
# List all the 9 rules by confidence level from high to low
selected_rules <- DATAFRAME(rules_1) %>%
arrange(desc(confidence))
recomm <- paste(selected_rules$LHS, "=>", selected_rules$RHS)
recomm
# Support and confidence values
support_levels <- c(0.0035, 0.003, 0.0025, 0.002)
confidence_levels <- seq(0.9, 0.65, by = -0.05)
# Create empty integers for each support level
library(hash)
d <- hash()
k <- 1
# Calculate the number of rules under different support and confidence level
for(i in support_levels) {
rules_count <- integer(length = length(confidence_levels))
for(j in 1:length(confidence_levels)) {
rules_count[j] <- length(apriori(df_tr, parameter = list(support=i, confidence=confidence_levels[j], target = "rules", minlen = 2)))
}
nam <- paste("rules_supp_", i, sep = "")
assign(nam, rules_count)
d[[toString(k)]] <- rules_count
k <- k + 1
}
recomm
install.packages("tinytex")
install.packages("rmarkdown")
install.packages("rmarkdown")
class(Boston)
data(Boston)
library(Boston)
library(MASS)
boston <- Boston
str(boston)
summary(boston)
dim(boston)
str(boston)
?Boston
head(boston)
boston$black <- NULL
set.seed(198761)
sample <- sample.int(n = nrow(boston), size = floor(0.75 * nrow(boston)))
train <- boston[sample, ]
test <- boston[-sample, ]
set.seed(198761)
sample <- sample.int(n = nrow(boston), size = floor(0.75 * nrow(boston)))
train <- boston[sample, ]
test <- boston[-sample,]
cor(train)
corplot(train)
cor(train)
library(ggplot)
library(ggplot2)
library(ggplot2)
ggplot(boston, aes(x=rm, y=medv)) + geom_point()
ggplot(boston, aes(x=rm, y=medv)) + geom_point() + geom_smooth(method = 'lm)
ggplot(boston, aes(x=rm, y=medv)) + geom_point() + geom_smooth(method = 'lm')
corplot(boston)
univariate_model <- lm(medv ~ rm, boston)
univariate_model <- lm(medv ~ rm, boston)
summary(univariate_model)
plot(univariate_model)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'blue', alpha = 0.5)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'dark blue', alpha = 0.5)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'dark blue', alpha = 0.5, bins = 40)
res <- residuals(univariate_model)
res <- as.data.frame(res)
ggplot(res, aes(res)) + geom_histogram(fill = 'dark blue', alpha = 0.5, binwidth = 2)
test$predicted.medv <- predict(univariate_model, test)
view(test)
test$predicted.medv <- predict(univariate_model, test)
library(dplyr)
view(test)
test$predicted.medv <- predict(univariate_model, test)
library(tidyverse)
view(test)
test$predicted.medv <- predict(univariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
multivariate_model <- lm(medv ~ ., train)
plot(multivariate_model)
test$predicted.medv <- predict.lm(multivariate_model, test)
view(test)
test$predicted.medv <- predict(multivariate_model, test)
view(test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
annova(univariate_model, multivariate_model)
anova(univariate_model, multivariate_model)
univariate_model <- lm(medv ~ rm, test)
summary(univariate_model)
test$predicted.medv <- predict(univariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
anova(univariate_model, multivariate_model)
anova(univariate_model, multivariate_model)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
test$predicted.medv <- predict(multivariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
anova(univariate_model, multivariate_model)
univariate_model <- lm(medv ~ rm, train)
summary(univariate_model)
test$predicted.medv <- predict(univariate_model, test)
error <- test$medv - test$predicted.medv
rmse <- sqrt(mean(error^2))
rmse
anova(univariate_model, multivariate_model)
m1 <- stepAIC(multivariate_model, direction = 'both)
m1 <- stepAIC(multivariate_model, direction = 'both')
d[[toString(k)]] <- rules_count
# Load required packages
library(dplyr)
library(arules)
library(arulesViz)
library(ggplot2)
# Load the data files
orders <- read.csv("order_products__prior.csv", nrows = 30000) # to reduce computability, we just load the first 30000 rows.
# randomly sample the rows
products <- read.csv("products.csv")
# Check the data structure
glimpse(orders)
glimpse(products)
# Inspect first few rows
head(orders)
head(products)
# Join the data on product_id
df <- orders %>%
inner_join(products, by = "product_id") %>%
group_by(order_id) %>%
summarise(basket = as.vector(list(product_name)))
# check the cleaned data
head(df)
# convert dataframe to sparse matrix for further analysis
df_tr <- as(df$basket, "transactions")
# check how many transactions and items
df_tr
# inspect first few transactions
inspect(head(df_tr))
# Display the distribution of number of items per basket
hist(size(df_tr), breaks = 0:80, xaxt = "n",
main = "Number of Items per Basket", xlab = "Number of Items", col = "#00ADB5")
axis(side = 1, at = seq(0, 80, by = 10), cex.axis = 0.8) # set x-axis label
mtext(paste("Total Baskets: ", length(df_tr)), line = -1) # set sub-title
itemFrequencyPlot(df_tr, topN = 15, type = "absolute", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Absolute")
itemFrequencyPlot(df_tr, topN = 15, type = "relative", col = "#00ADB5",
main = "Top 15 Most Frequent Items - Relative")
# use apriori function to get most frequent itemsets
ft_rules <- apriori(df_tr, parameter = list(target = "frequent itemsets", supp = 0.002, minlen = 2))
# inspect the rules by support from high to low
inspect(sort(ft_rules, by = "support", decreasing = T)[1:10]) # top 10 itemsets
# Plot the top 10 itemsets
plot(sort(ft_rules, by = "support", decreasing = T)[1:10], method = "graph", control = list(cex=0.6), main = "Top 10 Most Frequent Bought-Together Itemsets")
# Load packages
library(tidyverse)
# 2. Load data file
df <- read_csv("district_data.csv")
# check first a few rows
head(df)
# check the dimension of data
dim(df)
# check the summary of data
summary(df)
# check column names
names(df)
# rename the column names
names(df) <- c("district", "avg_income", "median_age")
names(df)
# remove $ and thousand separator
df$avg_income <- gsub("[$,]", "", df$avg_income)
# convert to numeric
df$avg_income <- as.numeric(df$avg_income)
# check the column types
glimpse(df)
qplot(x = df$median_age, y = df$avg_income, label = df$district, geom = "text")
df_scaled <- as.data.frame(scale(df[c("median_age", "avg_income")]))
head(df_scaled)
# define the number of clusters
k_clusters <- 3
# get clustering result
k_means_result <- kmeans(df_scaled, centers = k_clusters)
# add cluster result to original data
df$cluster <- as.factor(k_means_result$cluster)
head(df)
# visualize cluster result
qplot(x = df$median_age, y = df$avg_income, label = df$district, geom = "text", color = df$cluster)
# set maximum number of cluster
k_cluster_max <- 10
# iterate through the number of clusters from 1 to 10
within_sum_of_squares <- sapply(1:k_cluster_max,
function(k){
kmeans(df_scaled, centers = k)$tot.withinss})
# visualize the output
plot(1:k_cluster_max, within_sum_of_squares, type = "b",
xlab = "Number of clusters", ylab = "Total within cluster sum of squares")
# conduct clustering with 4 clusters
k_means_result_2 <- kmeans(df_scaled, centers = 4)
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# visualize cluster result with ggrepel to avoid label overlapping
library(ggrepel)
qplot(x = df$median_age, y = df$avg_income, label = df$district, color = df$cluster,
xlab = "Median Age", ylab = "Average Income") + geom_text_repel()
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# conduct clustering with 4 clusters
k_means_result_2 <- kmeans(df_scaled, centers = 4)
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# visualize cluster result with ggrepel to avoid label overlapping
library(ggrepel)
qplot(x = df$median_age, y = df$avg_income, label = df$district, color = df$cluster,
xlab = "Median Age", ylab = "Average Income") + geom_text_repel()
# conduct clustering with 4 clusters
k_means_result_2 <- kmeans(df_scaled, centers = 4)
# add result to dataframe
df$cluster <- as.factor(k_means_result_2$cluster)
# visualize cluster result with ggrepel to avoid label overlapping
library(ggrepel)
qplot(x = df$median_age, y = df$avg_income, label = df$district, color = df$cluster,
xlab = "Median Age", ylab = "Average Income") + geom_text_repel()
install.packages(c("arules", "clipr", "curl", "DT", "foreach", "foreign", "ggplot2", "hms", "httr", "iterators", "knitr", "markdown", "modelr", "nlme", "Rcpp", "rmarkdown", "rpart.plot", "seriation", "sys", "visNetwork", "whisker", "xfun", "xml2"))
install.packages(c("arules", "clipr", "curl", "DT", "foreach", "foreign", "ggplot2", "hms", "httr", "iterators", "knitr", "markdown", "modelr", "nlme", "Rcpp", "rmarkdown", "rpart.plot", "seriation", "sys", "visNetwork", "whisker", "xfun", "xml2"))
install.packages(c("arules", "ggplot2", "markdown"))
install.packages(c("DT", "httr"))
install.packages(c("clipr", "curl"))
install.packages(c("foreach", "foreign", "hms", "iterators", "knitr", "modelr", "nlme", "Rcpp", "rmarkdown", "rpart.plot", "seriation", "sys", "visNetwork", "whisker", "xfun", "xml2"))
library(keras)
install.packages("keras")
library(keras)
library(tidyverse)
setwd("~/GitHub/clp_data_analytic/neural_network")
install.packages("tensorflow")
install.packages("devtools")
library(keras)
library(tidyverse)
data <- read_csv("kc_house_data.csv")
summary(data)
data <- read_csv("kc_house_data.csv")
glimpse(data)
ID <- data$id
price <- data$price
data <- data %>% select(-c('price', 'id', 'date', 'zipcode', 'yr_built', 'condition', 'yr_renovated', 'lat', 'long', 'sqft_lot15'))
data <- data %>% cbind(ID) %>% cbind(price)
str(data)
model <- keras_model_sequential()
library(keras)
library(tidyverse)
install_tensorflow()
devtools::install_github("rstudio/tensorflow")
library(tensorflow)
install_tensorflow()
model <- keras_model_sequential()
model %>%
layer_dense(input_shape = ncol(x_train), activation = "tanh", kernel_initializer = "normal", units = 24) %>%
layer_dropout(rate = 0.04) %>%
layer_dense(units = 12, kernel_initializer = "normal") %>%
layer_activation_leaky_relu() %>%
layer_dense(units = 4, kernel_initializer = "normal") %>%
layer_activation_leaky_relu() %>%
layer_dropout(rate = 0.03) %>%
layer_dense(units = 12, kernel_initializer = "normal") %>%
layer_activation_leaky_relu() %>%
layer_dense(units = 24, kernel_initializer = "normal") %>%
layer_activation_leaky_relu() %>%
layer_dense(units = 1)
library(keras)
library(tidyverse)
data <- read_csv("kc_house_data.csv")
glimpse(data)
ID <- data$id
price <- data$price
data <- data %>% select(-c('price', 'id', 'date', 'zipcode', 'yr_built', 'condition', 'yr_renovated', 'lat', 'long', 'sqft_lot15'))
data <- data %>% cbind(ID) %>% cbind(price)
str(data)
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled %>% as.matrix(scale(x_train))
data <- read_csv("kc_house_data.csv")
glimpse(data)
ID <- data$id
price <- data$price
data <- data %>% select(-c('price', 'id', 'date', 'zipcode', 'yr_built', 'condition', 'yr_renovated', 'lat', 'long', 'sqft_lot15'))
data <- data %>% cbind(ID) %>% cbind(price)
str(data)
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled %>% as.matrix(scale(x_train))
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled <- as.matrix(scale(x_train))
x_test_scaled <- as.matrix(scale(x_test, center = attr(x_train_scaled, "scaled:center"),
scale=attr(x_train_scaled, "scale:scale")))
data <- read_csv("kc_house_data.csv")
glimpse(data)
ID <- data$id
price <- data$price
data <- data %>% select(-c('price', 'id', 'date', 'zipcode', 'yr_built', 'condition', 'yr_renovated', 'lat', 'long', 'sqft_lot15'))
data <- data %>% cbind(ID) %>% cbind(price)
str(data)
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled <- as.matrix(scale(x_train))
x_test_scaled <- as.matrix(scale(x_test, center = attr(x_train_scaled, "scaled:center"),
scale=attr(x_train_scaled, "scale:scale")))
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled <- as.matrix(scale(x_train))
x_test_scaled <- as.matrix(scale(x_test, center = attr(x_test_scaled, "scaled:center"),
scale=attr(x_test_scaled, "scale:scale")))
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled <- as.matrix(scale(x_train))
x_test_scaled <- as.matrix(scale(x_test, center = attr(x_test_scaled, "scaled:center"),
scale=attr(x_train_scaled, "scale:scale")))
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled <- as.matrix(scale(x_train))
x_test_scaled <- as.matrix(scale(x_test, center = attr(x_train_scaled, "scaled:center"),
scale=attr(x_train_scaled, "scale:scale")))
data <- read_csv("kc_house_data.csv")
glimpse(data)
ID <- data$id
price <- data$price
data <- data %>% select(-c('price', 'id', 'date', 'zipcode', 'yr_built', 'condition', 'yr_renovated', 'lat', 'long', 'sqft_lot15'))
data <- data %>% cbind(ID) %>% cbind(price)
str(data)
train <- data %>% sample_frac(size = 0.75)
test <- anti_join(data, train, by='ID')
x_train <- train %>% select(-c('ID', 'price'))
y_train <- train %>% select(c('price'))
x_test <- test %>% select(-c('ID', 'price'))
y_test <- test %>% select(c('price'))
x_train_scaled <- as.matrix(scale(x_train))
x_test_scaled <- as.matrix(scale(x_test, center = attr(x_train_scaled, "scaled:center"),
scale=attr(x_train_scaled, "scale:scale")))
